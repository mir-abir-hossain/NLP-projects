{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJVnefebv/OXLpXKXSoxE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mir-abir-hossain/NLP-projects/blob/main/Sentence_generation_using_ngram_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Goal of the project: Creating a bigram model from the Brown Corpus and evaluate the perplexity of the model"
      ],
      "metadata": {
        "id": "QsNGagiYrPMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Creating training and testing data from the brown corpus*"
      ],
      "metadata": {
        "id": "Zqi6O2hvrgJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmHzXCcAJ99D",
        "outputId": "ae942f9a-e828-4101-bc06-2cece1d70359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n",
        "categories = brown.categories()\n",
        "for i in categories:\n",
        "    print(i, end=', ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or22VOtEKFvw",
        "outputId": "ed464c0f-42f4-463b-e5f3-025d05dba600"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adventure, belles_lettres, editorial, fiction, government, hobbies, humor, learned, lore, mystery, news, religion, reviews, romance, science_fiction, "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = brown.sents(categories = ['adventure', 'fiction', 'science_fiction'])\n",
        "test_lines = brown.sents(categories = ['mystery'])"
      ],
      "metadata": {
        "id": "jeLpuDq4KipV"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_lines), len(test_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2__TrkMzMcXi",
        "outputId": "b5e889d8-543e-463e-94dc-da2db08b4832"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9834, 3886)"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_lines[:10], test_lines[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrG0r33oRGI",
        "outputId": "4390f3d3-5eb3-4d85-860e-de3977f74bdc"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Thirty-three'], ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.'], ['His', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'University', 'Hospital', '--', 'Mr.', 'McKinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.'], ['His', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.'], ['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.'], ['He', 'was', 'discharged', 'from', 'the', 'hospital', 'after', 'a', 'two-day', 'checkup', 'and', 'he', 'and', 'his', 'parents', 'had', 'what', 'Mr.', 'McKinley', 'described', 'as', 'a', '``', 'celebration', 'lunch', \"''\", 'at', 'the', 'cafeteria', 'on', 'the', 'campus', '.'], ['Rachel', 'wore', 'a', 'smart', 'hat', 'and', ',', 'because', 'she', 'had', 'been', 'warned', 'recently', 'about', 'smoking', ',', 'puffed', 'at', 'her', 'cigarettes', 'through', 'a', 'long', 'ivory', 'holder', 'stained', 'with', 'lipstick', '.'], [\"Scotty's\", 'father', 'sat', 'sprawled', 'in', 'his', 'chair', ',', 'angular', ',', 'alert', 'as', 'a', 'cricket', ',', 'looking', 'about', 'at', 'the', 'huge', 'stainless-steel', 'appointments', 'of', 'the', 'room', 'with', 'an', 'expression', 'of', 'proprietorship', '.'], ['Teachers', '--', 'men', 'who', 'wore', 'brown', 'suits', 'and', 'had', 'gray', 'hair', 'and', 'pleasant', 'smiles', '--', 'came', 'to', 'their', 'table', 'to', 'talk', 'shop', 'and', 'to', 'be', 'introduced', 'to', 'Scotty', 'and', 'Rachel', '.'], ['Rachel', 'was', 'polite', ',', 'Scotty', 'indifferent', '.']] [['There', 'were', 'thirty-eight', 'patients', 'on', 'the', 'bus', 'the', 'morning', 'I', 'left', 'for', 'Hanover', ',', 'most', 'of', 'them', 'disturbed', 'and', 'hallucinating', '.'], ['An', 'interne', ',', 'a', 'nurse', 'and', 'two', 'attendants', 'were', 'in', 'charge', 'of', 'us', '.'], ['I', 'felt', 'lonely', 'and', 'depressed', 'as', 'I', 'stared', 'out', 'the', 'bus', 'window', 'at', \"Chicago's\", 'grim', ',', 'dirty', 'West', 'Side', '.'], ['It', 'seemed', 'incredible', ',', 'as', 'I', 'listened', 'to', 'the', 'monotonous', 'drone', 'of', 'voices', 'and', 'smelled', 'the', 'fetid', 'odors', 'coming', 'from', 'the', 'patients', ',', 'that', 'technically', 'I', 'was', 'a', 'ward', 'of', 'the', 'state', 'of', 'Illinois', ',', 'going', 'to', 'a', 'hospital', 'for', 'the', 'mentally', 'ill', '.'], ['I', 'suddenly', 'thought', 'of', 'Mary', 'Jane', 'Brennan', ',', 'the', 'way', 'her', 'pretty', 'eyes', 'could', 'flash', 'with', 'anger', ',', 'her', 'quiet', 'competence', ',', 'the', 'gentleness', 'and', 'sweetness', 'that', 'lay', 'just', 'beneath', 'the', 'surface', 'of', 'her', 'defenses', '.'], ['We', 'had', 'become', 'good', 'friends', 'during', 'my', 'stay', 'at', 'Cook', 'County', 'Hospital', '.'], ['I', 'had', 'told', 'her', 'enough', 'about', 'myself', 'to', 'offset', 'somewhat', 'the', 'damaging', 'stories', 'that', 'had', 'appeared', 'in', 'local', 'newspapers', 'after', 'my', 'little', 'adventure', 'in', 'Marshall', 'Field', '&', 'Co.', '.'], ['She', 'knew', 'that', 'I', 'lived', 'at', 'a', 'good', 'address', 'on', 'the', 'Gold', 'Coast', ',', 'that', 'I', 'had', 'once', 'been', 'a', 'medical', 'student', 'and', 'was', 'thinking', 'of', 'returning', 'to', 'the', 'university', 'to', 'finish', 'my', 'medical', 'studies', '.'], ['She', 'knew', 'also', 'that', 'I', 'was', 'unmarried', 'and', 'without', 'a', 'single', 'known', 'relative', '.'], ['She', \"wasn't\", 'quite', 'sure', 'that', 'I', 'felt', 'enough', 'remorse', 'about', 'my', 'drinking', ',', 'or', 'that', 'I', 'would', 'not', 'return', 'to', 'it', 'once', 'I', 'was', 'out', 'and', 'on', 'my', 'own', 'again', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step 1: creating sentences from the lists and turn everything into lowercase*"
      ],
      "metadata": {
        "id": "ivzm__JKr7yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = [\" \".join(sent).lower() for sent in train_lines]\n",
        "test_sentences = [\" \".join(sent).lower() for sent in test_lines]"
      ],
      "metadata": {
        "id": "zNyE6h92q91s"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(train_sentences[:10]):\n",
        "    print(i, ': ', sent)\n",
        "print('----------------')\n",
        "for i, sent in enumerate(test_sentences[:10]):\n",
        "    print(i, ': ', sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytq6z8vbsuOj",
        "outputId": "7dd38f99-fb59-49d1-c1b9-9349b2b89267"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :  thirty-three\n",
            "1 :  scotty did not go back to school .\n",
            "2 :  his parents talked seriously and lengthily to their own doctor and to a specialist at the university hospital -- mr. mckinley was entitled to a discount for members of his family -- and it was decided it would be best for him to take the remainder of the term off , spend a lot of time in bed and , for the rest , do pretty much as he chose -- provided , of course , he chose to do nothing too exciting or too debilitating .\n",
            "3 :  his teacher and his school principal were conferred with and everyone agreed that , if he kept up with a certain amount of work at home , there was little danger of his losing a term .\n",
            "4 :  scotty accepted the decision with indifference and did not enter the arguments .\n",
            "5 :  he was discharged from the hospital after a two-day checkup and he and his parents had what mr. mckinley described as a `` celebration lunch '' at the cafeteria on the campus .\n",
            "6 :  rachel wore a smart hat and , because she had been warned recently about smoking , puffed at her cigarettes through a long ivory holder stained with lipstick .\n",
            "7 :  scotty's father sat sprawled in his chair , angular , alert as a cricket , looking about at the huge stainless-steel appointments of the room with an expression of proprietorship .\n",
            "8 :  teachers -- men who wore brown suits and had gray hair and pleasant smiles -- came to their table to talk shop and to be introduced to scotty and rachel .\n",
            "9 :  rachel was polite , scotty indifferent .\n",
            "----------------\n",
            "0 :  there were thirty-eight patients on the bus the morning i left for hanover , most of them disturbed and hallucinating .\n",
            "1 :  an interne , a nurse and two attendants were in charge of us .\n",
            "2 :  i felt lonely and depressed as i stared out the bus window at chicago's grim , dirty west side .\n",
            "3 :  it seemed incredible , as i listened to the monotonous drone of voices and smelled the fetid odors coming from the patients , that technically i was a ward of the state of illinois , going to a hospital for the mentally ill .\n",
            "4 :  i suddenly thought of mary jane brennan , the way her pretty eyes could flash with anger , her quiet competence , the gentleness and sweetness that lay just beneath the surface of her defenses .\n",
            "5 :  we had become good friends during my stay at cook county hospital .\n",
            "6 :  i had told her enough about myself to offset somewhat the damaging stories that had appeared in local newspapers after my little adventure in marshall field & co. .\n",
            "7 :  she knew that i lived at a good address on the gold coast , that i had once been a medical student and was thinking of returning to the university to finish my medical studies .\n",
            "8 :  she knew also that i was unmarried and without a single known relative .\n",
            "9 :  she wasn't quite sure that i felt enough remorse about my drinking , or that i would not return to it once i was out and on my own again .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 2: removing punctuation marks and adding start and end token for every sentence"
      ],
      "metadata": {
        "id": "zO9SKHGuw2Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def initial_preprocess(sentences):\n",
        "    sents = [sent.translate(str.maketrans('', '', string.punctuation)) for sent in sentences]\n",
        "    sents = ['<s> ' + sent + ' </s>' for sent in sents]\n",
        "    return sents\n",
        "\n",
        "preprocessed_train_sentences = initial_preprocess(train_sentences)\n",
        "preprocessed_test_sentences = initial_preprocess(test_sentences)"
      ],
      "metadata": {
        "id": "vYW0BNV0sgfr"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sent in enumerate(preprocessed_train_sentences[:7]):\n",
        "    print(i, ': ', sent)\n",
        "print('----------------')\n",
        "for i, sent in enumerate(preprocessed_test_sentences[:7]):\n",
        "    print(i, ': ', sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77tgN64ttPcz",
        "outputId": "c75bc6ef-973e-48cb-9972-e68580c2555d"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 :  <s> thirtythree </s>\n",
            "1 :  <s> scotty did not go back to school  </s>\n",
            "2 :  <s> his parents talked seriously and lengthily to their own doctor and to a specialist at the university hospital  mr mckinley was entitled to a discount for members of his family  and it was decided it would be best for him to take the remainder of the term off  spend a lot of time in bed and  for the rest  do pretty much as he chose  provided  of course  he chose to do nothing too exciting or too debilitating  </s>\n",
            "3 :  <s> his teacher and his school principal were conferred with and everyone agreed that  if he kept up with a certain amount of work at home  there was little danger of his losing a term  </s>\n",
            "4 :  <s> scotty accepted the decision with indifference and did not enter the arguments  </s>\n",
            "5 :  <s> he was discharged from the hospital after a twoday checkup and he and his parents had what mr mckinley described as a  celebration lunch  at the cafeteria on the campus  </s>\n",
            "6 :  <s> rachel wore a smart hat and  because she had been warned recently about smoking  puffed at her cigarettes through a long ivory holder stained with lipstick  </s>\n",
            "----------------\n",
            "0 :  <s> there were thirtyeight patients on the bus the morning i left for hanover  most of them disturbed and hallucinating  </s>\n",
            "1 :  <s> an interne  a nurse and two attendants were in charge of us  </s>\n",
            "2 :  <s> i felt lonely and depressed as i stared out the bus window at chicagos grim  dirty west side  </s>\n",
            "3 :  <s> it seemed incredible  as i listened to the monotonous drone of voices and smelled the fetid odors coming from the patients  that technically i was a ward of the state of illinois  going to a hospital for the mentally ill  </s>\n",
            "4 :  <s> i suddenly thought of mary jane brennan  the way her pretty eyes could flash with anger  her quiet competence  the gentleness and sweetness that lay just beneath the surface of her defenses  </s>\n",
            "5 :  <s> we had become good friends during my stay at cook county hospital  </s>\n",
            "6 :  <s> i had told her enough about myself to offset somewhat the damaging stories that had appeared in local newspapers after my little adventure in marshall field  co  </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 3: replace the words, that occurs only once in the corpus, with < UNK > token"
      ],
      "metadata": {
        "id": "3dz6BdTj5VAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "def generate_tokens(sentence):\n",
        "    \"\"\"\n",
        "    Takes a list of sentences with start and end tokens.\n",
        "    Replace the words which occur only once in the corpus with\n",
        "    '<UNK>' token and return the list of all tokens.\n",
        "    For example:\n",
        "    Args:\n",
        "        sentences(list):\n",
        "        ['<s> this is first sentence </s>', '<s> this is second sentence </s>']\n",
        "\n",
        "    Returns:\n",
        "        tokens_with_unk(list):\n",
        "        ['<s>', 'this', 'is', '<UNK>', 'sentence', '</s>', '<s>', 'this', 'is', '<UNK>', 'sentence', '</s>']\n",
        "\n",
        "    \"\"\"\n",
        "    unk = '<UNK>'\n",
        "    tokens = ' '.join(sentence).split()\n",
        "    vocab = FreqDist(tokens)\n",
        "    freq_one = [i for i in vocab.keys() if vocab[i]==1]\n",
        "    tokens_with_unk = []\n",
        "    for word in tokens:\n",
        "        if word in freq_one:\n",
        "            tokens_with_unk.append(unk)\n",
        "        else:\n",
        "            tokens_with_unk.append(word)\n",
        "\n",
        "    return tokens_with_unk\n",
        ""
      ],
      "metadata": {
        "id": "K4QuCPjatOyR"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokens = generate_tokens(preprocessed_train_sentences)\n",
        "test_tokens = generate_tokens(preprocessed_train_sentences)"
      ],
      "metadata": {
        "id": "FYLvLrRY8IR7"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step 4: Create n-grams using these tokens*"
      ],
      "metadata": {
        "id": "WcIVGaPNbajX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrams(tokens, n=2):\n",
        "    \"\"\"\n",
        "    Create n-grams and return unique n-grams with their corresponding counts.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): list of tokens\n",
        "        n(int) = 1 for unigram, 2 for bigram\n",
        "\n",
        "    Returns:\n",
        "    n-grams(dict): dictionary of n-grams as a tuple and it's corresponding count.\n",
        "\n",
        "    Example:\n",
        "        tokens = ['<s>', 'this', 'is', '<UNK>', 'sentence', '</s>',\n",
        "                '<s>', 'this', 'is', '<UNK>', 'sentence', '</s>']\n",
        "        For n = 2,\n",
        "        n_grams: {\n",
        "                ('<s>', 'this') : 2,\n",
        "                ('this', 'is') : 2,\n",
        "                ('is', '<UNK>') : 2,\n",
        "                ('<UNK>', 'sentence') : 2,\n",
        "                ('</s>' '<s>') : 1,\n",
        "                ('sentence', '</s>') : 2\n",
        "                }\n",
        "    \"\"\"\n",
        "    ngram_list = list(nltk.ngrams(tokens, n))\n",
        "    ngram_dicts = {}\n",
        "    for pairs in ngram_list:\n",
        "        if pairs not in ngram_dicts:\n",
        "            ngram_dicts[pairs] = 1\n",
        "        else:\n",
        "            ngram_dicts[pairs] += 1\n",
        "\n",
        "    return ngram_dicts\n"
      ],
      "metadata": {
        "id": "nZoQXrWZaYKk"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_dicts = ngrams(train_tokens, 2)\n",
        "unigram_dicts = ngrams(train_tokens, 1)"
      ],
      "metadata": {
        "id": "uCjd60tuqDYf"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in bigram_dicts.items():\n",
        "    if v > 250:\n",
        "        print(k, v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZM9loXhdFOl",
        "outputId": "1d563031-57f5-4cfa-979a-2aae47d5b61a"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('</s>', '<s>') 9833\n",
            "('and', '<UNK>') 326\n",
            "('a', '<UNK>') 437\n",
            "('at', 'the') 281\n",
            "('it', 'was') 320\n",
            "('of', 'the') 817\n",
            "('<UNK>', 'of') 345\n",
            "('<UNK>', '</s>') 998\n",
            "('<s>', 'he') 1090\n",
            "('he', 'was') 346\n",
            "('<UNK>', 'and') 431\n",
            "('on', 'the') 376\n",
            "('<UNK>', '<UNK>') 540\n",
            "('<s>', 'the') 871\n",
            "('the', '<UNK>') 919\n",
            "('<s>', 'i') 491\n",
            "('he', 'had') 336\n",
            "('in', 'the') 652\n",
            "('<s>', 'she') 309\n",
            "('<s>', 'it') 334\n",
            "('to', 'the') 391\n",
            "('<s>', 'but') 264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = FreqDist(train_tokens)\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouVviVwfcgyB",
        "outputId": "8e1f8f21-699f-4a99-95c8-1c6c4eb9962b"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6774"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA8jHXANefyn",
        "outputId": "dac4f230-340e-466a-dff7-8b15c6d31f5e"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'<s>': 9834, '</s>': 9834, 'the': 8295, '<UNK>': 7089, 'and': 3770, 'to': 3136, 'of': 3079, 'a': 3008, 'he': 2782, 'was': 2210, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 5: apply the laplace smoothing formula"
      ],
      "metadata": {
        "id": "ApwkWHp7xINZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def laplace_smoothed_bigram(bigram, bigram_count, unigram_dicts, vocab_size):\n",
        "    unigram = bigram[0]\n",
        "    unigram = tuple(list(unigram.split()))\n",
        "    if unigram in unigram_dicts.keys():\n",
        "        unigram_count = unigram_dicts[unigram]\n",
        "    else:\n",
        "        unigram_count = 0\n",
        "    smoothed_prob = (bigram_count + 1)/(unigram_count + vocab_size)\n",
        "\n",
        "    return smoothed_prob"
      ],
      "metadata": {
        "id": "qenE8gFBcy9b"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smoothing(bigram_dict):\n",
        "    return {n_gram: laplace_smoothed_bigram(n_gram, count, unigram_dicts, len(vocab)) \\\n",
        "            for n_gram, count in bigram_dicts.items()}"
      ],
      "metadata": {
        "id": "0z8MTTXjfvFl"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = smoothing(bigram_dicts)\n",
        "sorted(model.items(), key=lambda x: x[1], reverse=True)[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdRJx1shgKv7",
        "outputId": "b2bac5ec-6415-4c3a-a447-7cfac535e318"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('</s>', '<s>'), 0.5921242774566474),\n",
              " (('of', 'the'), 0.08302039987820968),\n",
              " (('in', 'the'), 0.07419611407794569),\n",
              " (('<UNK>', '</s>'), 0.07206232417225708),\n",
              " (('<s>', 'he'), 0.0656912331406551),\n",
              " (('the', '<UNK>'), 0.06105249187072798),\n",
              " (('<s>', 'the'), 0.05250481695568401),\n",
              " (('on', 'the'), 0.04843890530643711),\n",
              " (('a', '<UNK>'), 0.04477611940298507),\n",
              " (('to', 'the'), 0.03955600403632694),\n",
              " (('it', 'was'), 0.039261252446183954),\n",
              " (('<UNK>', '<UNK>'), 0.039024742119310396),\n",
              " (('at', 'the'), 0.03726215644820296),\n",
              " (('he', 'was'), 0.036312264545835075),\n",
              " (('he', 'had'), 0.035265801590623695),\n",
              " (('<UNK>', 'and'), 0.031162086128543605),\n",
              " (('and', '<UNK>'), 0.03101289833080425),\n",
              " (('him', '</s>'), 0.03014811901953074),\n",
              " (('<s>', 'i'), 0.029624277456647398),\n",
              " (('from', 'the'), 0.027986348122866895)]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step 6: Calculating perplexity*"
      ],
      "metadata": {
        "id": "nuu09PdPz4QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masks = [[1,1], [1, 0], [0, 1], [0, 0]]\n",
        "\n",
        "def convert_oov(ngram):\n",
        "    \"\"\"Converts, if necessary, a given n-gram to one which is known by the model.\n",
        "    Args:\n",
        "        ngram (tuple): a bigram tuple. for ex: (\"the\", \"great\")\n",
        "    Returns:\n",
        "        The n-gram with <UNK> tokens in certain positions such that the model\n",
        "        contains an entry for it.\n",
        "\n",
        "    \"\"\"\n",
        "    mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
        "\n",
        "    ngram = (ngram,) if type(ngram) is str else ngram\n",
        "    for possible_known in [mask(ngram, bitmask) for bitmask in masks]:\n",
        "        if possible_known in model:\n",
        "            return possible_known"
      ],
      "metadata": {
        "id": "e7sL8tCWgTBL"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ngrams = nltk.ngrams(test_tokens, 2)\n",
        "N = len(test_tokens)\n",
        "known_ngrams  = (convert_oov(ngram) for ngram in test_ngrams)\n",
        "probs = [model[ngram] for ngram in known_ngrams]"
      ],
      "metadata": {
        "id": "EbKBMPAgwNS6"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def perplexity(prob, N):\n",
        "    perplexity = np.exp(sum([np.log(x) for x in prob])*(-1)/N)\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "IpndqBGiwTS4"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pps = perplexity(probs, N)\n",
        "print(f\"Perplexity of the model is: {pps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXwK7PFuwfBj",
        "outputId": "2f3a53f8-a35d-45bd-bf39-0932f1e85a3c"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of the model is: 616.8423318232044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*step 7: Generate Sentence*"
      ],
      "metadata": {
        "id": "YYOEKumP0ERX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def best_candidate(prev, i, without=[]):\n",
        "    \"\"\"Choose the most likely next token given the previous (n-1) tokens.\n",
        "    Args:\n",
        "        prev (tuple of str): the previous n-1 tokens of the sentence.\n",
        "        i (int): which candidate to select if not the most probable one.\n",
        "        without (list of str): tokens to exclude from the candidates list.\n",
        "    Returns:\n",
        "        A tuple with the next most probable token and its corresponding probability.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    blacklist  = [\"<UNK>\"] + without\n",
        "    candidates = ((ngram[-1], prob) for ngram, prob in model.items() if ngram[:-1]==prev)\n",
        "    candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "    candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
        "\n",
        "    if len(candidates) == 0:\n",
        "        return (\"</s>\", 1)\n",
        "    else:\n",
        "        candidate_index = int((random.randint(0, len(candidates)))/2)\n",
        "        return candidates[candidate_index if prev != () and prev[-1] != \"<s>\" else i]\n",
        "\n",
        "def generate_sentences(num, min_len=12, max_len=24):\n",
        "    \"\"\"Generate random sentences using the language model.\n",
        "    Args:\n",
        "        num (int): the number of sentences to generate.\n",
        "        min_len (int): minimum allowed sentence length.\n",
        "        max_len (int): maximum allowed sentence length.\n",
        "    Yields:\n",
        "        A tuple with the generated sentence and the combined probability\n",
        "        (in log-space) of all of its n-grams.\n",
        "\n",
        "    \"\"\"\n",
        "    for i in range(num):\n",
        "        sent, prob = [\"<s>\"], 1\n",
        "        while sent[-1] != \"</s>\":\n",
        "            prev = tuple(sent[-(1):])\n",
        "            blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
        "            next_token, next_prob = best_candidate(prev, i, without=blacklist)\n",
        "            sent.append(next_token)\n",
        "            prob *= next_prob\n",
        "\n",
        "            if len(sent) >= max_len:\n",
        "                sent.append(\"</s>\")\n",
        "\n",
        "        yield ' '.join(sent), -1/math.log(prob)"
      ],
      "metadata": {
        "id": "I6svzByqwiiO"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating sentences...\")\n",
        "for sentence, prob in generate_sentences(num = 5):\n",
        "    print(\"{} ({:.5f})\".format(sentence, prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "172uZXNuwzRu",
        "outputId": "6131666a-df54-427b-cdc1-2747ff8ce569"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating sentences...\n",
            "<s> he opened a chinless jake called and rolled between his bones of this mood it from hardtack into battle andrei remembered to roll </s> (0.00575)\n",
            "<s> the protest from it will find a blanket much was of july </s> (0.01041)\n",
            "<s> i really die of study room in spite and learn if a little payne did so nice people which gives something heroic even </s> (0.00569)\n",
            "<s> it but when in four or out as your trial period mike turned on now i meant him an afternoons after wilson made </s> (0.00576)\n",
            "<s> she threw him be doing on by its estate people talk with importance to abel </s> (0.00820)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IS5GaL5cw0Xe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}